# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FVM68sWDG5q2ITR0FTKZtqDPWbtYvIcJ

âœ¨WELCOME TO âœ¨

                                          ðŸ¡ House Price Prediction Project ðŸ¡


                                                                                              BY :           

                                                                                                    KEERTHANA R
                                                                                                    Bsc cs (AI&ML)
                                                                                                    TU6243202111025

#1. Dataset Loading & Inspection ?

#Load Excel File:

Use pd.read_excel("file.xlsx")

#Example:
dataset = pd.read_excel("HousePricePrediction.xlsx")

#Check Info:

.info() â†’ column names, data types, null values.

#Example:
"LotArea" is int, "Neighborhood" is object.

#Check Missing Values:

.isnull().sum() â†’ counts NaNs per column.

#Example:
"GarageType" has 50 missing values.

#Why inspect?

To detect missing values, wrong types, outliers.

#Example:
"Price" should not be negative.

#SAMPLE CODE
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score
from statsmodels.stats.outliers_influence import variance_inflation_factor

"""---

#2. Feature Selection (X, y split) ?

#Independent (X) vs Dependent (y):

X â†’ inputs (size, location, rooms).

y â†’ target (SalePrice).

#Example:
Predict SalePrice using LotArea + Bedrooms.

#Why separate?

Models learn mapping from X â†’ y.

# SAMPLE CODE
"""

import pandas as pd
dataset = pd.read_excel("HousePricePrediction.xlsx")

print("âœ… Dataset Loaded Successfully")
print("Shape:", dataset.shape)
print(dataset.head())
print(dataset.info())
print(dataset.isnull().sum())

"""---

#3. Handling Categorical & Numerical Columns ?

#Detect Categorical:

select_dtypes(include=['object'])

#Example:
"Neighborhood", "HouseStyle".

#Detect Numerical:

select_dtypes(exclude=['object'])

#Example:
"LotArea", "YearBuilt".

#Why preprocess?

ML models need numbers, not text.

# SAMPLE CODE
"""

X = dataset.drop("SalePrice", axis=1)
y = dataset["SalePrice"]

cat_cols = X.select_dtypes(include=['object']).columns
num_cols = X.select_dtypes(include=['int64', 'float64']).columns

print("Categorical features:", list(cat_cols))
print("Numerical features:", list(num_cols))

preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), num_cols),
        ("cat", OneHotEncoder(drop='first', sparse_output=False), cat_cols)
    ]
)

"""---

#4. Data Preprocessing Pipelines ?

#Numerical Features:

Fill missing values â†’ SimpleImputer(strategy="mean")

Scale â†’ StandardScaler()

#Example:
If "LotArea" has NaN, fill with mean value.

#Categorical Features:

Fill missing values â†’ SimpleImputer(strategy="most_frequent")

Encode â†’ OneHotEncoder()

#Example:
"Neighborhood": A, B, C" â†’ three columns [1,0,0], [0,1,0], [0,0,1].

#Why ColumnTransformer?

Apply both num + cat preprocessing in one step.

#SAMPLE CODE
"""

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2,
                                                    random_state=42)

"""---

#5. Model Training (Linear Regression) and Model Evaluation ?

#Numerical Features:

Fill missing values â†’ SimpleImputer(strategy="mean")

Scale â†’ StandardScaler()

#Example:
If "LotArea" has NaN, fill with mean value.

#Categorical Features:

Fill missing values â†’ SimpleImputer(strategy="most_frequent")

Encode â†’ OneHotEncoder()

#Example:
"Neighborhood": A, B, C" â†’ three columns [1,0,0], [0,1,0], [0,0,1].

#Why ColumnTransformer?

Apply both num + cat preprocessing in one step.

#Model Evaluation

#RMSE (Root Mean Square Error):

Lower RMSE = better accuracy.

#Example:
RMSE = 25000 â†’ model is off by ~â‚¹25,000.

#RÂ² Score:

Explains variance captured by model.

#RÂ² = 0.9 â†’ 90% of price variation explained.

#SAMPLE CODE
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

dataset = dataset.replace(["?", "NA", "na", " ", " "], np.nan)

dataset = dataset.dropna(subset=["SalePrice"])

X = dataset.drop("SalePrice", axis=1)
y = dataset["SalePrice"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

cat_cols = X.select_dtypes(include=['object']).columns.tolist()
num_cols = X.select_dtypes(exclude=['object']).columns.tolist()

num_pipeline = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", StandardScaler())
])

cat_pipeline = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", num_pipeline, num_cols),
        ("cat", cat_pipeline, cat_cols)
    ],
    remainder="drop"
)

lin_reg = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", LinearRegression())
])

lin_reg.fit(X_train, y_train)

y_pred = lin_reg.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("âœ… Model trained successfully")
print("RMSE:", rmse)
print("RÂ² Score:", r2)

"""---

#7. Feature Importance

#Coefficients:
Show feature effect on target.

Positive coef â†’ increases price.

Negative coef â†’ decreases price.

#Example:
GarageArea +20000 â†’ each extra sq ft adds â‚¹20,000.

#Plot Top Features:
Use bar chart.

#SAMPLE CODE
"""

ohe_features = lin_reg.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(cat_cols)
all_features = np.concatenate([num_cols, ohe_features])

coef = lin_reg.named_steps['model'].coef_

importance = pd.DataFrame({"Feature": all_features, "Coefficient": coef})
importance = importance.sort_values(by="Coefficient", key=lambda x: abs(x), ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x="Coefficient", y="Feature", data=importance.head(15))
plt.title("Top 15 Features Influencing Price")
plt.show()

"""---

#8. Multicollinearity (VIF)

#Definition:
When features are correlated â†’ unstable model.

VIF > 10 â†’ serious multicollinearity.

#Example:
"GarageArea" & "GarageCars" are correlated â†’ high VIF.

#SAMPLE CODE
"""

X_processed = preprocessor.fit_transform(X)
X_processed = pd.DataFrame(X_processed, columns=all_features)

vif_data = pd.DataFrame()
vif_data["Feature"] = all_features
vif_data["VIF"] = [variance_inflation_factor(X_processed.values, i)
                   for i in range(X_processed.shape[1])]

print("Variance Inflation Factors:")
print(vif_data.sort_values("VIF", ascending=False).head(10))

"""---

#9. Polynomial Regression

#Adds non-linear features:

#Example:
Price = b0 + b1*Area + b2*(AreaÂ²)

#Advantage:
Captures curves.

#Risk:
High degree â†’ overfitting.

#SAMPLE CODE
"""

poly_preprocessor = ColumnTransformer(
    transformers=[
        ("num", Pipeline([
            ("scale", StandardScaler()),
            ("poly", PolynomialFeatures(degree=2, include_bias=False))
        ]), num_cols),
        ("cat", OneHotEncoder(drop='first', sparse_output=False), cat_cols)
    ]
)

poly_model = Pipeline(steps=[("preprocessor", poly_preprocessor),
                             ("model", LinearRegression())])

poly_model.fit(X_train, y_train)
y_pred_poly = poly_model.predict(X_test)

print("Polynomial RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_poly)))
print("Polynomial RÂ²:", r2_score(y_test, y_pred_poly))

"""---

#10. Outlier Detection

#Boxplot:
Detects values far outside normal range.

#Example:
"LotArea" usually < 20000, but one house has 200000 â†’ outlier.

#Impact:
Outliers distort predictions.

#Fix:
Remove or transform data.

#SAMPLE CODE
"""

plt.figure(figsize=(6,4))
sns.boxplot(x=y)
plt.title("House Price Distribution (Check Outliers)")
plt.show()

"""---

#11. Location Effect on Price

#Why Important?
Location strongly impacts price.

#Example:

House in "Downtown" â†’ â‚¹80L

Same size in "Village" â†’ â‚¹40L

Visualization: Boxplot of Price vs Location.

#SAMPLE CODE
"""

if "Location" in dataset.columns:
    plt.figure(figsize=(10,6))
    sns.boxplot(x="Location", y="Price", data=dataset)
    plt.xticks(rotation=45)
    plt.title("Effect of Location on House Price")
    plt.show()

"""---

#12. Residual Analysis (Assumptions Check)

Residual = Actual â€“ Predicted

#Checks model assumptions:

Random scatter â†’ good model.

#Example:
If residuals increase with price â†’ heteroscedasticity.

#Normality:
Residuals should form bell curve.

#SAMPLE CODE
"""

residuals = y_test - y_pred

plt.scatter(y_pred, residuals)
plt.axhline(y=0, color="red", linestyle="--")
plt.title("Residuals vs Predictions (Linearity & Homoscedasticity)")
plt.show()

sns.histplot(residuals, kde=True)
plt.title("Residual Distribution (Normality Check)")
plt.show()

"""# 13.Sample Load Dataset :"""

dataset = pd.read_excel("HousePricePrediction.xlsx")

print("âœ… Dataset Loaded Successfully")
print("Shape:", dataset.shape)
print(dataset.head())
print(dataset.info())
print(dataset.isnull().sum())

"""#15 . Conclusion of the Project :

#Feature Influence

Key factors like Lot Area, Overall Quality, Total Rooms, and Location strongly influence house prices.

Categorical variables (e.g., Neighborhood) also play a significant role after one-hot encoding.

#Model Performance

The Linear Regression model gave a good baseline with RMSE â‰ˆ value from your run and RÂ² â‰ˆ value.

Polynomial Regression slightly improved accuracy but increased complexity.

Regularization methods like Ridge/Lasso can handle multicollinearity and prevent overfitting.

#Impact of Preprocessing

Handling missing values, scaling numerical features, and encoding categorical variables were essential for stable model training.

Outliers significantly impact predictions; removing or transforming them improves results.

#Multicollinearity & Assumptions

Variance Inflation Factor (VIF) showed some correlated features; reducing them helped avoid unstable coefficients.

Linear regression assumptions (linearity, independence, homoscedasticity, and normality of residuals) were checked using residual plots.

#Practical Insights

Location is one of the most crucial drivers of house prices.

The model can be deployed for real-estate businesses to estimate fair house values.

For production, a more robust model (like Random Forest or Gradient Boosting) may perform better than plain Linear Regression.
"""

import pyfiglet

ascii_banner = pyfiglet.figlet_format("            THANK    YOU   !")
print(ascii_banner)